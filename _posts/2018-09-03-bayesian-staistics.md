---
title: Bayesian Statistics
description: Bayesian Statistics!
header: All things Bayesian!
tags: ['bayesian-statistics']
categories:  bayesian-statistics
permalink: /:categories/:title
---
## Why study Bayesian statistics?

So you want to catch data science bandwagon. In the process, may be, you are nicely catching up with probability distributions, getting comfortable cranking up linear models, hypothesis testings and statistical analysis / inference exercises. And may be you have even heard of Bayesian statistics before, heard of “frequentist” and “Bayesian” heated debates in data-science meet ups you attend. But still, does the talk of bayesian statistics make you uncomfortable? In this post, I will try to explain bayesian statistics, and it’s practicality in simple terms.

Is there any reason to use Bayesian statistics? My answer is - if you ever deal with data sparsity, there are clear advantages to using Bayesian statistics. It helps you to make clever decisions when you have “small” datasets to work with. It forces you to pay attention to uncertainty which otherwise can be easily overlooked in favor of quicker decision making. Let me explain! When you are estimating conversion rate, or comparing outcomes of two experiments or measuring incrementally, how often did you look at how much data you have at hand? How often you did sample study calculations? When you don’t have enough data, it is very tempting to do some quick and (hopefully) reasonable data plumbing, just so that you can take decisions quickly. If you do this, don’t shy away from Bayesian statistics. Courses / blog posts with Bayesian statistics often come with intimidating equations which often seem un-natural to think with. But don’t be afraid of those equations as well! Deep down, we all do use Bayesian statistics. If you don’t believe me, think of a time when you didn’t believe in a baseball player who hit multiple home runs in the very first match he played. (Or conversely, you were not worried when your star player, who usually hit home runs consistently, failed miserably in a league game). The point is - we very often let our intuition or beliefs skew data we observe. In the above baseball example, for our rookie player we “act” as if she hit 0 home runs. For our star player we act as if she hit home-runs even if she failed. This is because, deep down, we know that a single game doesn’t change our prior beliefs. Like it or not, we use Bayesian statistics without even knowing it!

Why it makes sense to study Bayesian statistics? Because it gives you principled probabilistic framework to combine your beliefs and data to help decisions you take. It forces you to look at every single metric you calculate or every parameter you estimate as a “random variable”. And once you recognize that things you measure are riddled with noise, you make better decisions considering uncertainty associated with it. Without Bayesian thinking, it is very easy to report and compare averages (and may be cite “central limit theorem” or hypothesis testing procedures without looking at variation / noise) and forget about with how much data you computed those averages with. With explicit treatment of priors and observed data, Bayesian statistics doesn’t let you forget that.

## What is Bayesian statistics?

![Real world vs probabilistic world](/img/prob_model_1.png?raw=true){:height="120%" width="120%"}

When you calculate a metric or fit a linear model, there is a probabilistic interpretation you sometimes don’t think of (I mean, if you are a statistician you always do, but then if you wouldn’t be reading this post, do you?). When you compute a conversion rate as total conversions over total clicks, the probabilistic interpretation tells us that by doing that we are really fitting a binomial distribution to our data (Figure 1). It just happens that the way you calculated conversion rate (i.e. conversion over clicks) exactly match with the maximum likelihood estimate of parameter of a binomial distribution (or poisson for that matter). Fitting a probabilistic distribution is exactly same as estimating parameters of the probability distribution which maximizes likelihood of parameters given observed data. This is the so called Maximum Likelihood Estimation of estimating parameters of a model. Fitting a probability distribution helps because only then you can infer or estimate something (e.g. how good your ad is, or what will conversion rate of the same ad tomorrow?).

Now if you have calculated conversion rate this way enough times, you know for sure that you will run in to cases when number of clicks will be very low. Without knowing anything about probability theory how do you deal with ads which have 1 or 2 clicks a day? You will say, that is easy, you probably count clicks and conversions over multiple days. You pool data such that you can trust that denominator in your conversion rate calculation. Or may be you do something more sophisticated and robust. You calculate a weighted average of campaign level conversion rate and ad level conversion rate. You use some function to compute a weight for your ad level estimate which decays with amount of clicks you see for an ad. You say this is perfect! Now my ad level conversion rate estimate is a perfect compromise between campaign level conversion rate and ad level conversion rate. Ever thought about a probabilistic interpretation of what you just did? My friend, you are already doing Bayesian statistics! 

![Real world vs probabilistic world](/img/prob_model_2.png?raw=true){:height="60%" width="60%"}

Look at that ever famous Bays rule which is very foundational to Bayesian statistics (Figure 2). Bayesians use posterior distribution which is product of prior distribution and likelihood function (normalized such a way that posterior distribution sums to 1) to infer about quantity of interest. This is very similar to what you did earlier by weighing ad level conversion rate (which is entirely based on data about that specific ad) by entire campaign’s historic conversion rate (which is what your first impression was about all ads in that campaign would behave). In fact, this weighing is very essence of Bayesian statistics. For inference and estimation, it weighs likelihood function (which is entirely based on data) with prior distribution (which represents your belief). Thats it! In doing so, it treats parameter of a probability distribution, which is really the metric of interest above (e.g. parameter of binomial), as a random variable. With Bayesian treatment, now your parameter of interest is always a compromise between data and prior. Once you have overwhelming data, it trumps prior but when data are sparse, the final estimate is governed by your prior.

In all Bayesian blogs / papers all those intimidating equations you see.. they are nothing but functional forms of prior, likelihood function or posterior distributions. As you suspect, math gets real ugly when you are actually trying to compute the posterior distribution. And this is why people use sampling approaches (MCMC, Hamiltonian Mote Carlo etc.) to approximate the posterior. And it works!

In nutshell, in Bayesian statistics:
1. Everything is a random variable, and there is uncertainty associated with everything you do.. this includes the hypothesis you test, quantities you compute and parameters you estimate!
2. You always form a prior distribution based on information you have about the hypothesis, quantity, parameter etc. before even you start observing the data
3. Once you observe some data, you update your opinion about the hypothesis, quantity or parameter. More formally, you combine prior distribution and likelihood using Bays rule, to compute the posterior distribution of random variable (which represents quantity of interest!)

## A very simple case of beta-binomial distribution!

Let’s look at very simple use-case of Bayesian Statistics, with the so called Beta-binomial distribution!
So lets get back to problem we discussed earlier. You have clicks and conversions of each ad, and now you want to estimate conversion rate of your ads. But you do worry that while some ads have 100s of clicks, there are a few ads which receive less than 10 clicks. You just don’t feel comfortable using your old method of conversions / clicks to estimate conversion rate for these ads.

Remember, conversion rate is a random variable. And after looking at enough histograms of conversion rates we are now convinced that it follows binomial distribution. So this estimating conversion rate problem is equivalent of estimating parameter of a binomial distribution. We know now that our old method of conversions / clicks is really the maximum likelihood estimate of binomial distribution! 

Under Bayesian philosophy, this parameter of binomial distribution itself should be treated as a random variable. Now what prior distribution should we pick such that we can easily calculate the posterior distribution? Let’s use a known result from the so called [conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior). When beta distribution is chosen as prior distribution and when likelihood function is a binomial distribution, the resulting posterior distribution ends up following beta distribution. And now how to decide parameters of this prior beta distribution? If you want to use historic data across all campaigns, you can use something called as empirical Bay’s approach for that. So once you figure out alpha and beta parameters of prior beta distribution, then computing ad level posterior and mean of that posterior are really simple. The mean of the posterior simply boils down to (alpha+conversions) / (alpha+beta+clicks). I encourage you to do this yourself, and see how nicely you move away from prior once you have enough clicks. And when there are very few clicks your final estimates sticks with the campaign level conversion rate estimate.

## Epilogue:
Probability distributions are wonderful. They deserve a separate blog post.They are as fundamental to statistics as data structures are to computer science. And in the business of data science, where you are constantly measuring, evaluating and estimating things, believe me, thinking in terms of probability distributions help a great deal. We need data, I mean you do data science after all! But if you ever deal with sparse data or cold-start situations, principles from Bayesian statistics come to the rescue. When you have limited data, no need to panic. If you can see past those intimidating equations, Bayesian statistics is your friend! Oh, and when you do have myriad of data, by all means, go explore the “deep” side!!